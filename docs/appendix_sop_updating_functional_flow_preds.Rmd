---
title: "Updating Functional Flow Predictions"
description: |
  Steps to use revised watershed data to generate functional flow predictions
author:
  - name: Ryan Peek 
    affiliation: Center for Watershed Sciences, UCD
    affiliation_url: https://watershed.ucdavis.edu
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, tidy = FALSE, message = FALSE, warning = FALSE)

library(knitr)
library(here)
library(sf)
suppressPackageStartupMessages(library(tidyverse))
library(tmap)

# catchments (revised)
catch <- read_rds(here("data_input/catchments_final_lshasta.rds"))
h10 <- read_rds(here("data_input/huc10_little_shasta.rds"))
gages <- read_rds(here("data_input/gages_little_shasta.rds"))
springs <- read_rds(here("data_input/springs_little_shasta.rds"))

# flowlines
lsh_flowline <- read_rds(here("data_input/final_flowlines_w_full_nhd_vaa.rds")) %>%
  # remove all the junk just get what we need
  select(comid, fromnode, tonode, divergence, ftype,
         areasqkm, lengthkm, gnis_id, hydroseq) %>%
  st_transform(4269)

lois <- lsh_flowline %>%
  filter(comid %in% c(3917946, 3917950, 3917198))

# see here for some more info/reading
## https://waterdata.usgs.gov/blog/nldi_update/
## https://usgs-r.github.io/nhdplusTools/articles/advanced_network.html

library(mapview)
mapviewOptions(fgb = FALSE)

```

This document outlines the process used to revise watershed and streamline delineations, re-generate watershed scale data used in functional flow models, process these data (calculate accumulation values for the catchment or watershed for each variable), and finally re-run the functional flow predictive model to generate revised flow predictions for the functional flow metrics.

# Revising a Watershed Delineation

As part of the CEFF case study of the Little Shasta watershed, we identified a number of inaccuracies associated with the NHD watershed and streamline delineation.
In part, this was because several canals were classified as streams, and thus were included in the watershed streamline delineation.
In particular, the canal which delivers water from Lake Shastina was routed as if it drained into the Little Shasta River.

To address inaccuracies in the NHD flowlines and catchments associated with designation of canals as streams, the following steps were taken:

1. We identified natural stream channel from artificial (canal) channels, and filtered all artificial segments out, and fixed mis-classified segments.
2. We identified catchments based on the new clean streamline dataset, and create a revised catchment layer, cropped to fall within the NHD HUC10 boundary. We attributed each catchment within the HUC10 to a flowline COMID from the cleaned flowline layer.
3. We calculated the total drainage area for each NHD COMID segment using the `{nhdtoolsPlus}` package and add this attribute information as a new column in the stream layer. Retain the old drainage area associated with each NHD segment to facilitate comparisons.


## Correcting HUC boundaries

Unfortunately the Little Shasta Watershed is incorrectly delineated. It includes canals and adjacent watershed catchments from the Lake Shastina area. A first step required removing these catchments and canals.

(ref:LSHboundary) *The pre-existing watershed boundary for the Little Shasta watershed.*

```{r lshbound, eval=TRUE, layout="l-page", echo=FALSE, out.width='50%', fig.cap='(ref:LSHboundary)'}

knitr::include_graphics(here("data_input/map_of_flowlines_existing_vs_cleaned.png"))

```

## Cleaned Flowlines and Catchments

We manually removed the canal and catchments that fell outside of the HUC 10 boundary. There are also several springs and sinks (river channels that end or remain disconnected from the mainstem) in this watershed. For Functional flow calculation we needed to generate a clean flow network and catchment map. 

```{r cleanMap, fig.cap="Cleaned Little Shasta Catchment Map.", layout="l-page"}

# List sink/isolated segments (N=19)
sinks <- c(3917228, 3917212, 3917214, 3917218, 3917220,
           3917960, 3917958, 3917276, 3917278, 3917274,
           3917282, 3917284, 3917286, 3917280, 3917268,
           3917256, 3917250, 3917272, 3917956)

# make just sinks
lsh_fsinks <- lsh_flowline %>% filter(comid %in% sinks)
lsh_fmain <- lsh_flowline %>% filter(!comid %in% sinks)

# preview
mapview(catch, col.regions="gray", color="gray20",
        alpha.regions=0.1, alpha=0.5,
        layer.name="Catchments <br>trimmed to H10") +
  mapview(lsh_fmain, zcol="hydroseq", legend=FALSE, layer.name="Revised Flowlines<br>(hydroseq)") +
  mapview(lsh_fsinks, color="purple", lwd=1, layer.name="Sinks") +
  mapview(lwgeom::st_endpoint(lois), col.regions="orange", cex=6, layer.name="LOI") +
  mapview(springs, col.regions="steelblue",color="skyblue", alpha.regions=0.8, cex=4, layer.name="Springs") +
  mapview(gages, col.regions="black", color="blue", lwd=2,cex=3, layer.name="Gages")

```

## Pull NHD Data and Update Catchments

We can use the following code to download the NHD flowlines and catchments with attributes (`vaa`). This uses the existing catchment set that we already have (from the HUC10).

```{r pullNHD, echo=TRUE, eval=FALSE}

# download all the flowlines and catchments
# grab based on COMIDs from catchments (could use lsh_flowline$comids)
nhd_vaa <- subset_nhdplus(comids = as.integer(catch_clean$FEATUREID),
                         output_file = "data_input/nhdplus_vaa.gpkg", # subset_file,
                         nhdplus_data = "download",
                         flowline_only = FALSE,
                         return_data = TRUE, overwrite = TRUE)

# view pieces
st_layers("data_input/nhdplus_vaa.gpkg")
```

Once we have a fully attributed NHD flowline, we can calculate distances and watershed areas.
The **arbolate** sum is the total length of all upstream flowlines. 
We use the cleaned NHD flowline network and calculated the `arbolatesum` using the [`nhdplusTools::calculate_arbolate_sum()`](https://usgs-r.github.io/nhdplusTools/reference/calculate_arbolate_sum.html) function.

```{r arbolate, echo=FALSE, eval=TRUE, fig.cap="Little Shasta delineated by total upstream length (wider line means greater distance upstream)", layout="l-page"}

# first we need to make sure lines are strings
#lsh_flowline_trim_lstring <- sf::st_cast(lsh_flowline_trim, "LINESTRING")

library(nhdplusTools)

# generate clean comid network
flownet <- get_tocomid(lsh_fmain, return_dendritic = TRUE, missing = 0, add = TRUE)

# regenerate/check flowline lengths:
flownet <- flownet %>%
  mutate(lengthkm_check = st_length(.)) %>%
  mutate(lengthkm_check = round(x = units::drop_units(lengthkm_check)/1000, 3))

# check and sort
flownet <- get_sorted(flownet, split = FALSE)
flownet['sort_order'] <- 1:nrow(flownet)
#plot(flownet['sort_order'])

# Rename and compute upstream lengths
flownet[["arbolatesum"]] <- calculate_arbolate_sum(
  dplyr::select(flownet,
                ID = comid, toID = tocomid, length = lengthkm_check))  
flownet <- flownet %>% 
  dplyr::mutate(plotlwd = arbolatesum / 10)

# plot based on upstream flowpath
plot(sf::st_geometry(flownet), lwd = flownet$plotlwd)

```

However, the catchment delineation is still fragmented, (because it was done previously with canals and artificial channels), and in many cases there are catchments without flowlines present. We can either drop these catchments, or we can manually assign them to flow via adjacent catchments to a single COMID. This revised catchment network can be used to regenerate accumulation data. Here we show the lower catchments that were assigned to a comid in the mainstem. All upper catchments have a 1:1 assignment to a catchment, no revision was required.

```{r getCatchNHD, echo=FALSE, eval=TRUE}

library(units)

# clean catch
# drop these catch gridcodes, they are all splinters
to_drop <- c(1386713,1387823, 1387701,1387917, 1387877,
             1387655, 1387682, 1387926, 1387559, 1387104,
             1387360, 1387795,1387830, 1520905, 1386208,1386450,
             1386396,1386475, 1386300,1386459,1386291, 1386638, 1386532,
             1386593, 1386796, 1387661, 1387838, 1387679, 1386893,
             1386339, 1387033)
# note 1387724 and 1387816 have some slivers/edges
# run with and without !GRIDCODE to see
catch_clean <- catch %>% filter(!GRIDCODE %in% to_drop) %>%
  # recalc areas?
  mutate(area2 = units::set_units(st_area(geom),"m^2") %>% set_units("km^2") %>% drop_units())
# select(catch_clean, comid, FEATUREID, AreaSqKM, area2) %>% View()

# get nhd flowlines
nhd_vaa <- sf::st_read(here("data_input/nhdplus_vaa.gpkg"), "NHDFlowline_Network", quiet=TRUE) %>%
  filter(comid %in% lsh_fmain$comid)

nhd_vaa_full <- sf::st_read(here("data_input/nhdplus_vaa.gpkg"), "NHDFlowline_Network", quiet=TRUE) %>%
  filter(comid %in% lsh_flowline$comid)

# get nhd catchments
catch_vaa <- sf::st_read(here("data_input/nhdplus_vaa.gpkg"), "CatchmentSP", quiet=TRUE)

# see table of catchs that were modified
#table(catch_clean$comid)
#3917070 3917072 3917082 3917084 3917106 3917114 3917130 3917136 3917138 3917154 
#      1       1       1       1       1       1       1       1       1       1 
#3917156 3917162 3917164 3917172 3917176 3917178 3917194 3917198 3917200 3917244 
#      1       1       1       1       1       1       1       4       2       5 
#3917912 3917914 3917916 3917918 3917946 3917948 3917950 
#      1       1       1       1       2      12      26 

# LOIs:  3917198 (3) 3917950 (2), 3917946 (1)

# get just the comids that were reassigned
catch_edited <- catch_clean %>% 
  filter(comid %in% c(3917200, 3917244, 3917198, 3917950, 3917948, 3917946)) %>% 
  mutate(comid_f = factor(comid_f))

# custom color pal
# library(qualpalr)
# pal <- qualpal(length(catch_edited), colorspace=list(h=c(0,360), s=c(0.3,1), l=c(0.2,0.8)))
# 
# mapview(lsh_flownet, lwd=3) + 
#   mapview(catch_clean, col.regions = "gray", color="gray30", alpha.regions=0.2, lwd=0.8) +
#   mapview(catch_edited, zcol="comid_f", legend=FALSE,
#           col.regions = pal$hex, color="gray20",
#           alpha.regions=0.8, lwd=2.3, alpha=0.8)
```


```{r makestaticcatchmap, echo=FALSE, eval=FALSE, fig.cap="Revised catchments associated with cleaned streamlines in the Little Shasta.", layout="l-page"}

library(tmap)
library(tmaptools)

tmap_mode("plot")
gm_osm <- read_osm(catch_clean, type = "esri-topo", raster=TRUE)

tm_shape(gm_osm) + tm_rgb() +
  tm_shape(catch_clean) + 
  tm_polygons(col="gray", border.lwd = 0.3, border.alpha = 0.4, alpha=0.2, border.col = "gray30") +
  tm_shape(catch_edited) + 
  tm_sf(col="comid_f", border.lwd = 2, alpha=0.7, border.col = "black", title = "COMID") +
  tm_shape(lois) + 
  tm_sf(col="navyblue", lwd=8) +
  tm_shape(flownet) + tm_lines(col="royalblue2", lwd=2) +
  tm_compass(type = "4star", position = c("left","bottom")) +
  tm_scale_bar(position = c("left","bottom")) +
  tm_layout(frame = FALSE,
            #legend.position = c("left", "top"), 
            title = 'Little Shasta Watershed', 
            #title.position = c('right', 'top'),
            legend.outside = FALSE, 
            attr.outside = FALSE,
            #fontfamily = "Roboto Condensed",
            #inner.margins = 0.01, outer.margins = (0.01),
            #legend.position = c(0.6,0.85),
            title.position = c(0.7, 0.95))

# works with roboto  
tmap::tmap_save(filename = "figs/tmap_lshasta.png", width = 10, height = 8, dpi=300)

# doesn't work with roboto
tmap::tmap_save(filename = "figs/tmap_lshasta.pdf", width = 10, height = 8, dpi=300)
# crop
#tinytex::tlmgr_install('pdfcrop')
knitr::plot_crop("figs/tmap_lshasta.pdf")
knitr::plot_crop("figs/tmap_lshasta.png")
```

```{r staticcatchmap, echo=FALSE, eval=TRUE, fig.cap="Revised catchments associated with cleaned streamlines in the Little Shasta.", layout="l-page"}

knitr::include_graphics(here("figs/tmap_lshasta.png"))

```

Alternatively, we could use a catchment flowline network that only occurs in catchments with surface water flowlines. That would look like this:

```{r surfCatchonly, echo=FALSE, eval=TRUE, layout="l-page"}

# trim to only catchments with streamlines
catch_flowline <- catch_vaa[nhd_vaa, ]
tmap_mode("plot")

tm_shape(catch_clean) + 
  tm_polygons(col="gray", border.lwd = 0.3, border.alpha = 0.4, alpha=0.2, border.col = "gray30") +
  tm_shape(catch_flowline) + 
  tm_sf(col="cyan4", border.lwd = 2, alpha=0.5, border.col = "black") +
  tm_shape(lois) + 
  tm_sf(col="gold2", lwd=8) +
  tm_shape(flownet) + tm_lines(col="blue2", lwd=2) +
  tm_compass(type = "4star", position = c("left","bottom")) +
  tm_scale_bar(position = c("left","bottom")) +
  tm_layout(frame = FALSE,
            #legend.position = c("left", "top"), 
            title = 'Catchments with Flowlines', 
            #title.position = c('right', 'top'),
            legend.outside = FALSE, 
            attr.outside = FALSE,
            fontfamily = "Roboto Condensed",
            #inner.margins = 0.01, outer.margins = (0.01),
            #legend.position = c(0.6,0.85),
            title.position = c(0.2, 0.95))

# works with roboto  
#tmap::tmap_save(filename = "figs/tmap_lshasta_w_borders.png", width = 10, height = 8, dpi=300)

# crop
#knitr::plot_crop("figs/tmap_lshasta_no_borders.png")
```


# Creating an Accumulation Network

Now that we have a re-delineated catchment map, we still need to create a clean flowline/catchment network that can be used to run accumulation code. We also need to update the drainage areas and make sure we have accurately reflected each catchment and COMID before we can have an accurate accumulation network.

Here we will use **only** catchments that contain existing flow lines, to simplify the process and predictions. When or if it is necessary, we can go back and add additional catchments in. There are `r nrow(catch_flowline)` catchments and NHD segments that intersect, and `r nrow(catch_vaa)` total catchments in the HUC10 boundary of the Little Shasta.

Here we show how to recalculate the catchment area and total drainage area.

```{r catchArea, echo=FALSE, eval=TRUE, layout="l-body-outset"}

# need to recalculate totdasqkm (TotDASqKM)
library(nhdplusTools)

# then add in the comid routing (from catch_clean)
catch_vaa <- left_join(catch_vaa, 
                           st_drop_geometry(catch_clean) %>%
                             select(featureid=FEATUREID,
                                    comid_riv=comid, upper)) %>% 
  mutate(upper = ifelse(is.na(upper), FALSE, upper))  
  ## if need to calc area again:
  # st_transform(., 3310) %>% 
  # mutate(area_rev = st_area(geom) %>% set_units("km^2") %>% drop_units(), .after=areasqkm)

# prep data and then run accumulation for catchment area
catchment_area <- prepare_nhdplus(nhd_vaa, 0, 0,
                                  purge_non_dendritic = FALSE, warn = FALSE) %>%
  # add back in the correct areas to use in the calculation
  left_join(st_drop_geometry(catch_vaa) %>% 
              select(featureid, area_rev=areasqkm), by=c("COMID"="featureid")) %>% 
  select(ID = COMID, toID = toCOMID, area=area_rev)

# calc total drainage area
catchment_area <- catchment_area %>% 
  mutate(totda = calculate_total_drainage_area(.),
         # add previous calc
         nhdptotda = nhd_vaa$totdasqkm)

# now add back to lsh_flownet
flownet <- left_join(flownet, catchment_area,
                           by=c("comid"="ID"))
# preview table
flownet %>% st_drop_geometry() %>% 
  select(comid:ftype, areasqkm, lengthkm, sort_order, arbolatesum, totda) %>% 
  kable()  

```


## Accumulation Data

We previously pulled data required for the accumulation ([here](https://github.com/ryanpeek/ffm_accumulation)) variables used in the FF models. 

We calculate this based on an area weighted average, which is the area of a given COMID catchment, divided by the total watershed area. So the sum of `variable * [(local_catch_area) / (total_watershed_da)]`. In this case the area weight is the local catchment area divided by the total watershed area.

The original data and crosswalked information is in this [file on github](https://github.com/ryanpeek/ffm_accumulation/blob/main/data_clean/08_accumulated_final_xwalk.csv). There are approximately 250+ variables in this dataset. We need to join all these variables to the updated network and catchment areas.

Here we join the areas, calculate the total areas of each catchment, and then plot a single variable as an example.

```{r catchDat, echo=TRUE, eval=TRUE, fig.cap="Catchment data example.", out.width="80%"}

catch_dat <- read_csv("https://github.com/ryanpeek/ffm_accumulation/blob/main/data_clean/07_final_catchment_data_for_accum.csv?raw=true") %>% 
  # filter to years of interest
  filter(!wa_yr %in% c(1945:1949))

# filter and match with nhd comids
catch_dat <- filter(catch_dat, comid %in% catch_vaa$featureid) %>% 
  # add updated areas
  left_join(., catchment_area, by=c("comid"="ID")) %>% 
  select(-toID, -nhdptotda) %>% 
  relocate(any_of(c("area","totda")), .after=comid)

# check rows match
# nrow(catch_vaa)==length(unique(catch_dat$comid))
# max watershed area for seelcted catchments?
lsh_tot_da_area <- max(catchment_area$totda)

# quick map
catch_flowline %>%
  left_join(catch_dat %>% filter(wa_yr==2011),
            by=c("featureid"="comid")) %>%
  tm_shape(.) + tm_sf(col="run_apr_wy", title="2011 Runoff: Apr (mm)") +
  #tm_compass(type = "4star", position = c("left","bottom")) +
  #tm_scale_bar(position = c("left","bottom")) +
  tm_layout(frame = FALSE,
            legend.outside = FALSE, 
            attr.outside = FALSE)

catch_dat <- catch_dat %>% 
  mutate(area_weight = area/lsh_tot_da_area, .after=area)

```

We use the total watershed area (`r round(lsh_tot_da_area)` km^2) to calculate our area weights, which collectively sum to 1 for our watershed area.

## NLDI Routing

For every COMID, we want to have routing the extends to every COMID upstream (or upstream to downstream). This approach let's us list all the COMIDs associated with any starting COMID.

```{r nldiNetwork, echo=FALSE, eval=FALSE}

library(nhdplusTools)

flownet_accum <- prepare_nhdplus(nhd_vaa, 0, 0, 0, purge_non_dendritic = TRUE, warn = TRUE) %>% 
  select(-TotDASqKM) %>% 
  # add the revised tot drainage area:
  left_join(., st_drop_geometry(flownet) %>% select(comid, sort_order, arbolatesum, totda), by= c("COMID"="comid"))
  
## to pull the specific flowlines from a COMID use the following,
## however this will pull NHD layers from online, and WILL NOT
## not use local cleaned network

# nldi_nwis <- list(featureSource = "comid", featureID = 3917154)
# navigate_nldi(nldi_feature = nldi_nwis,
#               distance_km = 500,
#               mode = "UT")$UT %>% #View() 
#   st_drop_geometry() %>% View() # to just view comids
#  #st_geometry() %>% mapview() # to plot

# navigate the network and return COMIDs from a local network
network_ls <- nhd_vaa %>% 
  left_join(st_drop_geometry(flownet) %>% 
              select(sort_order, comid), by="comid") %>% 
  st_drop_geometry() %>% 
  arrange(desc(sort_order)) %>% pull(comid) %>% as.list()

# get list of comids for network
network_nav_ls <- map(network_ls, 
                      ~navigate_network(start = .x, 
                                        network = nhd_vaa, mode = "UT",
                                        distance_km = 500, 
                                        output = "comid") %>%
                        select(comid, hydroseq, dnhydroseq, uphydroseq) %>% 
                        st_drop_geometry())

# pull out just comids for accumulation?
network_df <- tibble("startCOMID"=map_int(network_ls, ~.x),
          "comids"=map(network_nav_ls, ~(.x[["comid"]]))) %>% 
  dplyr::group_by(startCOMID) %>% 
  mutate(comid_ls = paste(unlist(comids), sep='', collapse=', ')) %>% 
  ungroup()

# make a long version
network_df_long <- network_df %>% select(-comid_ls) %>% unnest(comids)

# write out:
network_df %>%
  select(-comids) %>%
  write_csv(., file = here("output/lsh_network_comids_trimmed.csv"))

write_rds(network_df, here("output/lsh_network_comids_trimmed.rds"))

rm(network_df_long, network_ls, network_nav_ls, network_df)

```

```{r getNetworkdf, echo=FALSE, eval=TRUE}

network_df <- read_rds(here("output/lsh_network_comids_trimmed.rds"))

```

## Calculate Accumulation

First we need to use the cross walk to identify which variables require what type of calculation for the accumulation. Most will use the area weighted average, some require a min or max. 

```{r accumVarSel}

# crosswalk of variables
xwalk <- read_csv(here("output/08_accumulated_final_xwalk.csv"))

vars_awa <- xwalk %>% 
  filter(accum_op_class == "AWA") %>% 
  select(mod_input_final) %>% 
  mutate(dat_output = case_when(
    mod_input_final == "ann_min_precip_basin" ~ "cat_minp6190",
    mod_input_final == "ann_max_precip_basin" ~ "cat_maxp6190",
    mod_input_final == "cat_ppt7100_ann" ~ "pptavg_basin",
    mod_input_final == "et_basin" ~ "cat_et",
    mod_input_final == "wtdepave" ~ "cat_wtdep",
    mod_input_final == "ann_min_precip" ~ "cat_minp6190",
    mod_input_final == "ann_max_precip" ~ "cat_maxp6190",
    mod_input_final == "pet_basin" ~ "cat_pet", 
    mod_input_final == "rh_basin" ~ "cat_rh",
    mod_input_final == "pptavg_basin" ~"cat_ppt7100_ann",
    TRUE ~ mod_input_final))

# filter to vars
cat_df_awa <- catch_dat %>%
  select(comid:wa_yr, vars_awa$dat_output)
  
  # drop the non info vars
  select(comidwa_yr, varnames_awa$dat_output) %>%
  select(comid:wa_yr, comid_flowline:upper, everything())

```


```{r accumCalc}



# create flownetwork for accumulation
flownet_accum <- prepare_nhdplus(nhd_vaa, 0, 0, 0, purge_non_dendritic = TRUE, warn = TRUE) %>% 
  select(-TotDASqKM) %>% 
  # add the revised tot drainage area:
  left_join(., st_drop_geometry(catch_vaa_rev) %>% select(featureid, TotDASqKM=areaSum, areasqkm), by= c("COMID"="featureid"))  

# need to change flownet_accum names
flownet_accum <- flownet_accum %>% 
  rename(ID = COMID, toID = toCOMID)

# accum downstream
accumulate_downstream <- function(x, var) {

  try(x <- st_drop_geometry(x), silent = TRUE)
  cat_order <- select(x, .data$ID)
  x[["toID"]] <- tidyr::replace_na(x[["toID"]], 0)
  x <- get_sorted(x)
  x[["toID_row"]] <- match(x[["toID"]], x[["ID"]])
  var_out <- x[[var]]
  if(any(is.na(x[[var]]))) {
    warning("NA values found, accumulation math may fail.")
  }
  toid_row <- x[["toID_row"]]
  for(cat in 1:length(var_out)) {
    var_out[toid_row[cat]] <- var_out[toid_row[cat]] + var_out[cat]
  }
  x[[var]] <- var_out
  x <- distinct(left_join(cat_order, x, by = "ID"))
  return(x[[var]])
}

lsh_accum <- select(lsh_flownet, ID = comid, toID = tocomid, AreaSqKM=areasqkm, LENGTHKM=lengthkm, Hydroseq=hydroseq) %>% 
  left_join(catch_dat, by = c("ID" = "comid"))
 
tst <- nhdplusTools:::accumulate_downstream(lsh_accum, var = "AreaSqKM")
tst
```




